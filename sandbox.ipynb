{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA_mat(data, with_privacy, sanitizer, sigma, n_components=60):\n",
    "    data = data.view(-1, data.size(1) * data.size(2) * data.size(3))\n",
    "    normalized_data = torch.t(torch.t(data) / torch.norm(data, p=2, dim=1))\n",
    "    covar = torch.mm(torch.t(normalized_data), normalized_data)\n",
    "    num_examples = data.size(0)\n",
    "    if with_privacy:\n",
    "        saned_covar = sanitizer.sanitize(torch.reshape(covar, (1,-1)), \n",
    "                                         sigma=sigma, \n",
    "                                         option=ClipOption(1.0, False), \n",
    "                                         num_examples=num_examples)\n",
    "        saned_covar = torch.reshape(saned_covar, covar.size())\n",
    "        saned_covar = 0.5 * (saned_covar + torch.t(saned_covar))\n",
    "    else:\n",
    "        saned_covar = covar\n",
    "    eigvals, eigvecs = torch.symeig(saned_covar, eigenvectors=True)\n",
    "    _, topk_indices = torch.topk(eigvals, n_components)\n",
    "    topk_indices = torch.reshape(topk_indices, (n_components,))\n",
    "    return torch.t(torch.index_select(torch.t(eigvecs), 0, topk_indices))\n",
    "\n",
    "# def PCA_mat(data, with_privacy, sanitizer, n_components=60):\n",
    "#     data = data.view(-1, data.size(1) * data.size(2) * data.size(3))\n",
    "#     data_mean = torch.mean(data, 0)\n",
    "#     data = data - data_mean.expand_as(data)\n",
    "#     U, _, _ = torch.svd(torch.t(data))\n",
    "#     return U[:, :n_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, U):\n",
    "        super(Net, self).__init__()\n",
    "        self.U = U\n",
    "        self.fc1 = nn.Linear(60, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))\n",
    "        x = torch.mm(x, self.U)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "ClipOption = collections.namedtuple(\"ClipOption\",\n",
    "                                    [\"l2norm_bound\", \"clip\"])\n",
    "\n",
    "def BatchClipByL2norm(t, upper_bound):\n",
    "    batch_size = t.size(0)\n",
    "    t2 = torch.reshape(t, (batch_size, -1))\n",
    "    tensor = torch.tensor([])\n",
    "    upper_bound_inv = tensor.new_full((batch_size,), 1.0/upper_bound).to(device)\n",
    "    l2norm_inv = torch.rsqrt(torch.sum(t2*t2, 1) + 0.000001).to(device)\n",
    "    scale = torch.min(upper_bound_inv, l2norm_inv) * upper_bound\n",
    "    clipped_t = torch.mm(torch.diag(scale), t2)\n",
    "    clipped_t = torch.reshape(clipped_t, t.size())\n",
    "    return clipped_t\n",
    "\n",
    "def AddGaussianNoise(t, sigma):\n",
    "#     print(t)\n",
    "    if isinstance(t, torch.cuda.FloatTensor):\n",
    "        noisy_t = t + torch.normal(mean=torch.zeros(t.size()), std=sigma).to(device)\n",
    "    else:\n",
    "        noisy_t = t + torch.normal(mean=torch.zeros(t.size()), std=sigma)\n",
    "#     print(noisy_t)\n",
    "    return noisy_t\n",
    "\n",
    "class AmortizedGaussianSanitizer(object):\n",
    "    def __init__(self, accountant, default_option):\n",
    "        self.accountant = accountant\n",
    "        self.default_option = default_option\n",
    "    \n",
    "    def sanitize(self, x, sigma, option=ClipOption(None, None), num_examples=None, add_noise=True):\n",
    "        l2norm_bound, clip = option\n",
    "#         if len(x.size()) < 3:\n",
    "#             x.unsqueeze_(0)\n",
    "#         print('x')\n",
    "#         print(x)\n",
    "        if l2norm_bound is None:\n",
    "            l2norm_bound, clip = self.default_option\n",
    "        l2norm_bound_ = torch.tensor(l2norm_bound).to(device)\n",
    "        if clip:\n",
    "            x = BatchClipByL2norm(x, l2norm_bound_)\n",
    "#             print('clip_x')\n",
    "#             print(x)\n",
    "        if add_noise:\n",
    "            self.accountant.accumulate_privacy_spending(sigma, num_examples)\n",
    "#             saned_x = AddGaussianNoise(torch.sum(x, 0), \n",
    "            saned_x = AddGaussianNoise(x, \n",
    "                                       sigma * l2norm_bound)\n",
    "        else:\n",
    "#             saned_x = torch.sum(x, 0)\n",
    "            saned_x = x\n",
    "#         print('saned_x')\n",
    "#         print(saned_x)\n",
    "\n",
    "        return saned_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "\n",
    "EpsDelta = collections.namedtuple(\"EpsDelta\", [\"spent_eps\", \"spent_delta\"])\n",
    "\n",
    "def GenerateBinomialTable(m):\n",
    "    table = np.zeros((m + 1, m + 1), dtype=np.float64)\n",
    "    for i in range(m + 1):\n",
    "        table[i, 0] = 1\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            v = table[i - 1, j] + table[i - 1, j -1]\n",
    "            assert not math.isnan(v) and not math.isinf(v)\n",
    "            table[i, j] = v\n",
    "    return torch.from_numpy(table)\n",
    "\n",
    "class GaussianMomentsAccountant(object):\n",
    "    def __init__(self, total_examples, moment_orders=32):\n",
    "        self.total_examples = total_examples\n",
    "        self.moment_orders = range(1, moment_orders+1)\n",
    "        self.max_moment_order = max(self.moment_orders)\n",
    "        self.log_moments = torch.zeros(self.max_moment_order, dtype=torch.float64)\n",
    "        self.binomial_table = GenerateBinomialTable(self.max_moment_order)\n",
    "        \n",
    "    def accumulate_privacy_spending(self, sigma, num_examples):\n",
    "        q = num_examples * 1.0 / self.total_examples\n",
    "        for i in range(self.max_moment_order):\n",
    "            moment = self.compute_log_moment(sigma, q, self.moment_orders[i])\n",
    "            self.log_moments[i].add_(moment)\n",
    "    \n",
    "    def compute_log_moment(self, sigma, q, moment_order):\n",
    "        binomial_table = self.binomial_table[moment_order:moment_order+1, :moment_order+1]\n",
    "        qs = torch.exp(torch.tensor([i * 1.0 for i in range(moment_order+1)], \n",
    "                                    dtype=torch.float64) * torch.log(torch.tensor(q, dtype=torch.float64)))\n",
    "        moments0 = self.differential_moments(sigma, 0.0, moment_order)\n",
    "        term0 = torch.sum(binomial_table * qs * moments0)\n",
    "        moments1 = self.differential_moments(sigma, 1.0, moment_order)\n",
    "        term1 = torch.sum(binomial_table * qs * moments1)\n",
    "        return torch.log(q * term0 + (1.0 - q) * term1)\n",
    "    \n",
    "    def differential_moments(self, sigma, s, t):\n",
    "        binomial = self.binomial_table[:t+1, :t+1]\n",
    "        signs = np.zeros((t + 1, t + 1), dtype=np.float64)\n",
    "        for i in range(t+1):\n",
    "            for j in range(t+1):\n",
    "                signs[i, j] = 1.0 - 2 * ((i-j) % 2)\n",
    "        exponents = torch.tensor([i * (i + 1.0 - 2.0 * s) / (2.0 * sigma * sigma) \n",
    "                                  for i in range(t+1)], dtype=torch.float64)\n",
    "        x = torch.mul(binomial, torch.from_numpy(signs))\n",
    "        y = torch.mul(x, torch.exp(exponents))\n",
    "        z = torch.sum(y, 1)\n",
    "        return z\n",
    "    \n",
    "    def get_privacy_spent(self, target_deltas):\n",
    "        eps_deltas = []\n",
    "        for delta in target_deltas:\n",
    "            log_moments_with_order = zip(self.moment_orders, self.log_moments)\n",
    "            eps_deltas.append(EpsDelta(self.compute_eps(log_moments_with_order, delta), delta))\n",
    "        return eps_deltas\n",
    "    \n",
    "    def compute_eps(self, log_moments, delta):\n",
    "        min_eps = float(\"inf\")\n",
    "        for moment_order, log_moment in log_moments:\n",
    "            if math.isinf(log_moment) or math.isnan(log_moment):\n",
    "                continue\n",
    "            min_eps = min(min_eps, (log_moment - math.log(delta)) / moment_order)\n",
    "        return min_eps\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DPSGD(optim.SGD):\n",
    "    def __init__(self, sanitizer, sigma, batches_per_lot, params, lr, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False):\n",
    "        super(DPSGD, self).__init__(params, lr, momentum, dampening, weight_decay, nesterov)\n",
    "        self.batches_per_lot = batches_per_lot  # assume 1\n",
    "        self.grad_accum_dict = {}\n",
    "        self.sanitizer = sanitizer\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def compute_sanitized_gradients(self, loss, add_noise=True):\n",
    "        px_grads = loss  # TODO: per_example_gradients.\n",
    "                         # now assumes batch_size = 1\n",
    "        sanitized_grads = self.sanitizer.sanitize(px_grads, self.sigma, add_noise=add_noise, num_examples=self.batches_per_lot * px_grads.size(0))\n",
    "        \n",
    "        return sanitized_grads\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        '''\n",
    "        override step method\n",
    "        '''\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \"\"\"\n",
    "                modified the line below\n",
    "                old: d_p = p.grad.data\n",
    "                \"\"\"\n",
    "                d_p = self.compute_sanitized_gradients(p.grad.data)\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "                if momentum != 0:\n",
    "                    param_state = self.state[p]\n",
    "                    if 'momentum_buffer' not in param_state:\n",
    "                        buf = param_state['momentum_buffer'] = torch.zeros_like(p.data)\n",
    "                        buf.mul_(momentum).add_(d_p)\n",
    "                    else:\n",
    "                        buf = param_state['momentum_buffer']\n",
    "                        buf.mul_(momentum).add_(1 - dampening, d_p)\n",
    "                    if nesterov:\n",
    "                        d_p = d_p.add(momentum, buf)\n",
    "                    else:\n",
    "                        d_p = buf\n",
    "\n",
    "                p.data.add_(-group['lr'], d_p)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    if with_privacy:\n",
    "        spent_eps_deltas = priv_accountant.get_privacy_spent(target_deltas=[target_delta])\n",
    "        for spent_eps, spent_delta in spent_eps_deltas:\n",
    "            print(\"spent privacy: eps %.4f delta %.5g\\n\" % (\n",
    "              spent_eps, spent_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "test_batch_size = 1000\n",
    "epochs = 1\n",
    "lr = 0.01\n",
    "momentum = 0.0\n",
    "log_interval = 100\n",
    "\n",
    "with_privacy = True\n",
    "sigma = 1.0 #4.0\n",
    "pca_sigma = 1.0 #7.0\n",
    "target_delta = 1e-5\n",
    "batches_per_lot = 1\n",
    "default_gradient_l2norm_bound = 4.0 #4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.545932\n",
      "Train Epoch: 1 [100/60000 (0%)]\tLoss: 0.416265\n",
      "Train Epoch: 1 [200/60000 (0%)]\tLoss: 0.008158\n",
      "Train Epoch: 1 [300/60000 (0%)]\tLoss: 2.422462\n",
      "Train Epoch: 1 [400/60000 (1%)]\tLoss: 0.034336\n",
      "Train Epoch: 1 [500/60000 (1%)]\tLoss: 0.101162\n",
      "Train Epoch: 1 [600/60000 (1%)]\tLoss: 0.006901\n",
      "Train Epoch: 1 [700/60000 (1%)]\tLoss: 0.000217\n",
      "Train Epoch: 1 [800/60000 (1%)]\tLoss: 1.854816\n",
      "Train Epoch: 1 [900/60000 (2%)]\tLoss: 0.003532\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 0.000027\n",
      "Train Epoch: 1 [1100/60000 (2%)]\tLoss: 0.000473\n",
      "Train Epoch: 1 [1200/60000 (2%)]\tLoss: 0.188248\n",
      "Train Epoch: 1 [1300/60000 (2%)]\tLoss: 0.003960\n",
      "Train Epoch: 1 [1400/60000 (2%)]\tLoss: 0.002960\n",
      "Train Epoch: 1 [1500/60000 (2%)]\tLoss: 0.000416\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 1.699947\n",
      "Train Epoch: 1 [1700/60000 (3%)]\tLoss: 0.000950\n",
      "Train Epoch: 1 [1800/60000 (3%)]\tLoss: 0.174328\n",
      "Train Epoch: 1 [1900/60000 (3%)]\tLoss: 0.110695\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.003910\n",
      "Train Epoch: 1 [2100/60000 (4%)]\tLoss: 0.000610\n",
      "Train Epoch: 1 [2200/60000 (4%)]\tLoss: 0.007462\n",
      "Train Epoch: 1 [2300/60000 (4%)]\tLoss: 0.001305\n",
      "Train Epoch: 1 [2400/60000 (4%)]\tLoss: 0.000702\n",
      "Train Epoch: 1 [2500/60000 (4%)]\tLoss: 0.005081\n",
      "Train Epoch: 1 [2600/60000 (4%)]\tLoss: 0.000347\n",
      "Train Epoch: 1 [2700/60000 (4%)]\tLoss: 5.836128\n",
      "Train Epoch: 1 [2800/60000 (5%)]\tLoss: 4.557877\n",
      "Train Epoch: 1 [2900/60000 (5%)]\tLoss: 0.006149\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 0.007431\n",
      "Train Epoch: 1 [3100/60000 (5%)]\tLoss: 0.000137\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.003960\n",
      "Train Epoch: 1 [3300/60000 (6%)]\tLoss: 2.475258\n",
      "Train Epoch: 1 [3400/60000 (6%)]\tLoss: 0.000999\n",
      "Train Epoch: 1 [3500/60000 (6%)]\tLoss: 0.042477\n",
      "Train Epoch: 1 [3600/60000 (6%)]\tLoss: 0.000221\n",
      "Train Epoch: 1 [3700/60000 (6%)]\tLoss: 0.718330\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tLoss: 0.000824\n",
      "Train Epoch: 1 [3900/60000 (6%)]\tLoss: 0.000038\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.002220\n",
      "Train Epoch: 1 [4100/60000 (7%)]\tLoss: 0.025063\n",
      "Train Epoch: 1 [4200/60000 (7%)]\tLoss: 0.000435\n",
      "Train Epoch: 1 [4300/60000 (7%)]\tLoss: 0.538654\n",
      "Train Epoch: 1 [4400/60000 (7%)]\tLoss: 0.000343\n",
      "Train Epoch: 1 [4500/60000 (8%)]\tLoss: 0.032562\n",
      "Train Epoch: 1 [4600/60000 (8%)]\tLoss: 0.003960\n",
      "Train Epoch: 1 [4700/60000 (8%)]\tLoss: 0.035233\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.004807\n",
      "Train Epoch: 1 [4900/60000 (8%)]\tLoss: 0.059044\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.141319\n",
      "Train Epoch: 1 [5100/60000 (8%)]\tLoss: 0.002350\n",
      "Train Epoch: 1 [5200/60000 (9%)]\tLoss: 0.000015\n",
      "Train Epoch: 1 [5300/60000 (9%)]\tLoss: 0.022270\n",
      "Train Epoch: 1 [5400/60000 (9%)]\tLoss: 0.127163\n",
      "Train Epoch: 1 [5500/60000 (9%)]\tLoss: 0.000042\n",
      "Train Epoch: 1 [5600/60000 (9%)]\tLoss: 0.000610\n",
      "Train Epoch: 1 [5700/60000 (10%)]\tLoss: 0.001911\n",
      "Train Epoch: 1 [5800/60000 (10%)]\tLoss: 0.012985\n",
      "Train Epoch: 1 [5900/60000 (10%)]\tLoss: 0.001038\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.000458\n",
      "Train Epoch: 1 [6100/60000 (10%)]\tLoss: 0.001968\n",
      "Train Epoch: 1 [6200/60000 (10%)]\tLoss: 0.056080\n",
      "Train Epoch: 1 [6300/60000 (10%)]\tLoss: 0.000221\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.027908\n",
      "Train Epoch: 1 [6500/60000 (11%)]\tLoss: 0.003948\n",
      "Train Epoch: 1 [6600/60000 (11%)]\tLoss: 2.561203\n",
      "Train Epoch: 1 [6700/60000 (11%)]\tLoss: 0.000282\n",
      "Train Epoch: 1 [6800/60000 (11%)]\tLoss: 3.124474\n",
      "Train Epoch: 1 [6900/60000 (12%)]\tLoss: 0.025681\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 0.000565\n",
      "Train Epoch: 1 [7100/60000 (12%)]\tLoss: 0.015030\n",
      "Train Epoch: 1 [7200/60000 (12%)]\tLoss: 0.000454\n",
      "Train Epoch: 1 [7300/60000 (12%)]\tLoss: 0.000122\n",
      "Train Epoch: 1 [7400/60000 (12%)]\tLoss: 0.000999\n",
      "Train Epoch: 1 [7500/60000 (12%)]\tLoss: 0.000648\n",
      "Train Epoch: 1 [7600/60000 (13%)]\tLoss: 0.000191\n",
      "Train Epoch: 1 [7700/60000 (13%)]\tLoss: 0.001030\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tLoss: 2.365582\n",
      "Train Epoch: 1 [7900/60000 (13%)]\tLoss: 0.000198\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.019592\n",
      "Train Epoch: 1 [8100/60000 (14%)]\tLoss: 0.115177\n",
      "Train Epoch: 1 [8200/60000 (14%)]\tLoss: 0.019493\n",
      "Train Epoch: 1 [8300/60000 (14%)]\tLoss: 0.000435\n",
      "Train Epoch: 1 [8400/60000 (14%)]\tLoss: 0.001602\n",
      "Train Epoch: 1 [8500/60000 (14%)]\tLoss: 0.000019\n",
      "Train Epoch: 1 [8600/60000 (14%)]\tLoss: 0.253105\n",
      "Train Epoch: 1 [8700/60000 (14%)]\tLoss: 0.000038\n",
      "Train Epoch: 1 [8800/60000 (15%)]\tLoss: 0.000195\n",
      "Train Epoch: 1 [8900/60000 (15%)]\tLoss: 0.009644\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 0.224232\n",
      "Train Epoch: 1 [9100/60000 (15%)]\tLoss: 0.002495\n",
      "Train Epoch: 1 [9200/60000 (15%)]\tLoss: 0.165779\n",
      "Train Epoch: 1 [9300/60000 (16%)]\tLoss: 0.001724\n",
      "Train Epoch: 1 [9400/60000 (16%)]\tLoss: 0.000092\n",
      "Train Epoch: 1 [9500/60000 (16%)]\tLoss: 0.138260\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.013573\n",
      "Train Epoch: 1 [9700/60000 (16%)]\tLoss: 0.001400\n",
      "Train Epoch: 1 [9800/60000 (16%)]\tLoss: 0.000431\n",
      "Train Epoch: 1 [9900/60000 (16%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.000141\n",
      "\n",
      "Test set: Average loss: 0.2043, Accuracy: 9402/10000 (94%)\n",
      "\n",
      "spent privacy: eps inf delta 1e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "priv_accountant = GaussianMomentsAccountant(60000)\n",
    "gaussian_sanitizer = AmortizedGaussianSanitizer(\n",
    "        priv_accountant,\n",
    "        [default_gradient_l2norm_bound / batch_size, True])\n",
    "\n",
    "# pca init\n",
    "pca_train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=60000, shuffle=True)\n",
    "all_data, _ = iter(pca_train_loader).next()\n",
    "U = PCA_mat(all_data, with_privacy, gaussian_sanitizer, pca_sigma).to(device)\n",
    "\n",
    "model = Net(U).to(device)\n",
    "if with_privacy:\n",
    "    optimizer = DPSGD(sanitizer=gaussian_sanitizer, sigma=sigma, batches_per_lot=batches_per_lot, params=model.parameters(), lr=lr, momentum=momentum)\n",
    "else:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
